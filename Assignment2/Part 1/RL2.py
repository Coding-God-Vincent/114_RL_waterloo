import numpy as np
import MDP
from scipy.stats import beta
from tqdm import tqdm

class RL2:
    def __init__(self, mdp, sampleReward):
        '''Constructor for the RL class

        Inputs:
        mdp -- Markov decision process (T, R, discount) (It's an object)
        sampleReward -- Function to sample rewards (e.g., bernoulli, Gaussian).
        This function takes one argument: the mean of the distributon and 
        returns a sample from the distribution. (It's a function which can sample from a distribution)
        '''

        self.mdp = mdp
        self.sampleReward = sampleReward

#----------------------------------------------------------------------------------------------------------------------------------------------------------------#
    def sampleRewardAndNextState(self, state, action):
        '''Procedure to sample a reward and the next state
        reward ~ Pr(r)
        nextState ~ Pr(s'|s,a)

        Inputs:
        state -- current state
        action -- action to be executed

        Outputs: 
        reward -- sampled reward
        nextState -- sampled next state
        '''

        reward = self.sampleReward(self.mdp.R[action, state])
        cumProb = np.cumsum(self.mdp.T[action, state,:])
        nextState = np.where(cumProb >= np.random.rand(1))[0][0]
        return [reward, nextState]
    
    #----------------------------------------------------------------------------------------------------------------------------------------------------------------#
    # 這不是用在 bandit，是要用在 Maze 的
    # 這邊跟 qlearning function 統一只做一個 episode，不去做 100 trials。到 TestRL2Maze.py 再去做 trials 次的平均
    def modelBasedRL(self, s0, defaultT, initialR, nEpisodes, nSteps, epsilon= 0):
        '''Model-based Reinforcement Learning with epsilon greedy 
        exploration.  This function should use value iteration,
        policy iteration or modified policy iteration to update the policy at each step

        使用 Policy Iteration from MDP 來做。
        這邊的 model 並非一開始就已知。而是要先與環境互動，先學到環境的模型 (Reward, State Transition)，
        在透過學到的模型當作已知模型做 model-based RL。其實是用 model-based 的做法來做 model-free 的情況。
        流程如下：
        1. 與環境互動後得到經驗 (s, a, r, s') by epsilon-greedy (exploitation 的部分用我們估計的 R & P 所得的 policy (這邊使用的是 Policy Iteration))
        2. 利用統計的技巧得出一個估計的 R & P。
            2-1. R(r | s, a) = total(R(s, a)) / n((s, a)) (所有 (s, a) 得到的獎勵 / 出現 (s, a) 的次數)
            2-2. P(s' | s, a) = n(s' | s, a) / n((s, a)) (所有從 (s, a) 到 s' 的次數  / 出現 (s, a) 的次數)
        由上，不斷輪很多個 episode，最後 R & P 會越來越準。
        
        跟 Model-free 不同的地方在於：
        * Model-Free 學的是 "策略"。不去理解環境長怎樣，只知道要怎麼得高分。
        * Model-Based 學的是 "環境"，理解整個環境是怎樣之後再根據這個環境去產生策略後得到高分。
        
        這邊的 ModelBasedRL 是邊學習環境邊與環境互動，可以發現隨著環境的預測越來越準，我們每次互動所得到的獎勵也越來越高。
        
        Inputs:
        s0 -- initial state
        defaultT -- default transition function when a state-action pair has not been vsited。shape (nAction, nStates, nStates)
        initialR -- initial estimate of the reward function。shape (nActions, nStates) = (2, 4)
        nEpisodes -- # of episodes (one episode consists of a trajectory of nSteps that starts in s0
        nSteps -- # of steps per episode
        epsilon -- probability with which an action is chosen at random

        Outputs: 
        V -- final value function
        policy -- final policy
        cumulative_discounted_reward_episode -- cumulative discounted reward in each episode
        '''
        
        # cumulative_discounted_reward in each epsiode
        cumulative_discounted_reward_episode = np.zeros(nEpisodes)
        # init. model (will be updated continuously)
        T_, R_ = defaultT.copy(), initialR.copy()
        # data structure used to learn the model (by simple statistics)
        no_sa_pair = np.zeros((self.mdp.nActions, self.mdp.nStates))  # no. of (a, s)
        r_sa_pair = np.zeros((self.mdp.nActions, self.mdp.nStates))  # no. of (r | a, s)
        ns_sa_pair = np.zeros((self.mdp.nActions, self.mdp.nStates, self.mdp.nStates))  # no. of (ns | a, s)
        # init. policy
        policy = np.zeros(self.mdp.nStates, dtype= np.int64)
        
        for i in range(nEpisodes):
            # init. state
            state = s0
            # cumulative discounted reward in an episode
            total_reward = 0
            for s in range(nSteps):
                # choose action by epsilon-greedy
                if np.random.rand() < epsilon: action = np.random.choice(self.mdp.nActions, size= 1)[0]
                else: action = policy[state]  # use the policy generated by policy iteration based on the current R_ & T_
                # implement action and get reward & ns
                [reward, next_state] = self.sampleRewardAndNextState(state= state, action= action)
                total_reward += reward * np.pow(self.mdp.discount, s)
                # update T_ & R_
                no_sa_pair[action][state] += 1
                ns_sa_pair[action][state][next_state] += 1
                r_sa_pair[action][state] += reward
                T_[action][state] = ns_sa_pair[action][state] / no_sa_pair[action][state]
                R_[action][state] = r_sa_pair[action][state] / no_sa_pair[action][state]
                # state transition
                state = next_state
            
            # 每隔一個 episode 更新一次策略
            # 因為 maze 狀態比較多，使用 Policy Iteration 會因為運算量過大而動不了
            # use modified Policy Iteration to generate the current policy based on the current R_ & T_
            mdp_ = MDP.MDP(T= T_, R= R_, discount= self.mdp.discount)
            [policy, V, iterId, tolerance] = mdp_.modifiedPolicyIteration(initialPolicy= policy, initialV= np.zeros(self.mdp.nStates))
            cumulative_discounted_reward_episode[i] = total_reward
            
        return [V, policy, cumulative_discounted_reward_episode]   
    
    #----------------------------------------------------------------------------------------------------------------------------------------------------------------#
    # 從這邊開始往下都是用在 bandit
    def epsilonGreedyBandit(self, nIterations):
        '''Epsilon greedy algorithm for bandits (assume no discount factor).  Use epsilon = 1 / # of iterations.
           return the average (1000 trials (nIterations)) of the reward per iteration.
        
        ** don't mention how many steps in each iteration, so assume 1 step in each iteration

        Inputs:
        nIterations -- # of arms that are pulled

        Outputs: 
        rewards_earned -- total rewards earned in each iteration
        '''
        
        trials = 1000
        rewards_earned = np.zeros(nIterations)  # rewards earned in each Iteration

        for _ in tqdm(range(trials)):
            empiricalMeans = np.zeros(self.mdp.nActions)  # reward earned by each arm / implemented no. of each arm
            n_arm = np.zeros(self.mdp.nActions)  # implemented no. of each arm
            state = 0  # starts from state 0

            for i in range(nIterations):
                epsilon = 1 / (i+1)
                # choosing action by epsilon-greedy
                if np.random.rand(1) < epsilon:  # choose action randomly
                    action = np.random.choice(self.mdp.nActions, (1))
                else:  # choose action with the highest empiricalMean
                    action = np.argmax(empiricalMeans, axis= 0)
                n_arm[action] += 1
                    
                # get reward & next state
                [reward, next_state] = self.sampleRewardAndNextState(state= state, action= action)

                # update empiricalMeans of the recent action
                empiricalMeans[action] = (empiricalMeans[action] * (n_arm[action] - 1) + reward) / n_arm[action]

                # update state & rewards_earned
                state = next_state
                rewards_earned[i] += reward

        return rewards_earned / trials

    #----------------------------------------------------------------------------------------------------------------------------------------------------------------#
    def thompsonSamplingBandit(self, prior, nIterations, k= 1):
        '''Thompson sampling algorithm for Bernoulli bandits (assume no discount factor)

        ** don't mention how many steps in each iteration, so assume 1 step in each iteration

        每一個動作對應一個 beta 分布，代表著該動作的信心。在判斷動作的成功率時，我們會對其 beta 分布進行取樣，取出的值即為該動作的成功率。
        後驗分布怎麼做 : 
            每次成功我們會在 alpha + 1，使該分布較容易取出較高的機率。
            每次失敗我們會在 beta + 1，使該分布較容易取出較低的機率。

        Inputs:
        prior (shape (nAction, 2)) -- initial beta distribution over the average reward of each arm (|A|x2 matrix such that prior[a,0] is the alpha hyperparameter for arm a and prior[a,1] is the beta hyperparameter for arm a)  
                                      init: alpha = 1, beta = 1
                                            reward = 1 -> alpha + 1
                                            reward = 0 -> beta + 1
        nIterations -- # of arms that are pulled
        k -- # of sampled average rewards # 每 k 次平均一遍

        Outputs: 
        empiricalMeans -- empirical average of rewards for each arm (array of |A| entries)
        '''
        
        trials = 1000
        rewards_earned = np.zeros(nIterations)  # rewards earned in each iteration
        winning_probs = np.zeros(self.mdp.nActions)  # winning probs of each actions in 1 iteration
        
        for _ in tqdm(range(trials)):
            init_prior = prior.copy()
            state = 0
            for i in range(nIterations):
                
                # choosing actions by thompsonSampling
                for a in range(self.mdp.nActions):
                    # sample winning probs from corresponding beta distribution
                    winning_probs[a] = beta.rvs(a= init_prior[a][0], b= init_prior[a][1], size= k)  
                # current action is the action with the largest winning probs
                action = np.argmax(winning_probs, axis= 0)
                
                # get reward & next_state
                [reward, next_state] = self.sampleRewardAndNextState(state= state, action= action)
                
                # update the beta distribution of the current action
                if reward: init_prior[action][0] += 1
                else: init_prior[action][1] += 1

                rewards_earned[i] += reward
        
        return rewards_earned / trials
    
    #----------------------------------------------------------------------------------------------------------------------------------------------------------------#
    def UCBbandit(self, nIterations):
        '''Upper confidence bound algorithm for bandits (assume no discount factor)

        ** don't mention how many steps in each iteration, so assume 1 step in each iteration

        每次決定動作之前會先計算各動作的 UCB 值，最後挑選最大 UCB 值得動作來用。
        UCB 的計算公式 : UCB_i(t) = empiricalMean_i(t) + c * Sqrt(ln_t / no_actions_i(t))。c 常設定為 1 or Sqrt(2) (在這邊我用的是 Sqrt(2))
        可以看到 no_actions 在除數，所以不能為 0，因此要熱機，各動作先做一次。

        Inputs:
        nIterations -- # of arms that are pulled

        Outputs: 
        empiricalMeans -- empirical average of rewards for each arm (array of |A| entries)
        '''

        trials = 1000
        rewards_earned = np.zeros(nIterations)

        for _ in range(trials):
            empiricalMeans = np.zeros(self.mdp.nActions)  # empirical means of rewards of each arm in a single trial
            no_actions = np.zeros(self.mdp.nActions)  # no of actions of each arm in a single trial
            UCB_actions = np.zeros(self.mdp.nActions)
            state = 0
            
            # warm-up : implement each action for 1 time avoiding /0
            # the first three times of nIterations
            for a in range(self.mdp.nActions):
                no_actions[a] += 1
                [reward, next_state] = self.sampleRewardAndNextState(state= state, action= a)
                rewards_earned[a] += reward
                empiricalMeans[a] += reward
            
            # substract the no of the warm-up steps
            for i in range(3, nIterations):
                # choose action by UCB value
                for a in range(self.mdp.nActions):
                    # if i == 0, sqrt(log(0)) = nan, use 1 instead
                    if i == 0: UCB_actions[a] = empiricalMeans[a] + np.sqrt(2 * np.log(1) / no_actions[a])
                    else: UCB_actions[a] = empiricalMeans[a] + np.sqrt(2 * np.log(i) / no_actions[a])
                    
                action = np.argmax(UCB_actions, axis= 0).item()
                no_actions[action] += 1

                # implement action
                [reward, next_state] = self.sampleRewardAndNextState(state= state, action= action)

                # update empiricalMeans[action]
                empiricalMeans[action] = (empiricalMeans[action] * (no_actions[action] - 1) + reward) / no_actions[action] 

                rewards_earned[i] += reward

                
        return rewards_earned / trials